# Minutes IQ â€“ Multi-URL Client Architecture Update

## ğŸ“Œ Problem Statement

The current schema assumes each client has a single `website_url`.

Real-world use cases (e.g., JEA) require:
- One URL for current meetings
- One URL for archived meetings
- Potentially additional URLs for boards, committees, procurement, etc.

Therefore:

**A client must support multiple URLs, each with its own alias and lifecycle.**

---

# ğŸ¯ Architectural Decision

Replace single `website_url` field with a dedicated `client_urls` table.

This introduces a proper 1-to-many relationship:

- One client â†’ Many URLs

This supports:
- Multiple scrape targets per client
- URL-level tracking
- Future scheduling & analytics
- Cleaner relational design

---

# ğŸ§± Proposed Database Schema

## 1ï¸âƒ£ `clients` Table

| Field | Type | Notes |
|-------|------|-------|
| id | PK | Primary key |
| name | string | Client name |
| description | string | Optional |
| is_active | boolean | Soft enable/disable |
| created_at | timestamp | |
| updated_at | timestamp | |

Remove:
- âŒ `website_url`

---

## 2ï¸âƒ£ `client_urls` Table

| Field | Type | Notes |
|-------|------|-------|
| id | PK | Primary key |
| client_id | FK â†’ clients.id | Relationship |
| alias | string | e.g. "current", "archive", "board meetings" |
| url | string | Actual URL |
| is_active | boolean | Enables/disables scraping |
| last_scraped_at | timestamp | Optional |
| created_at | timestamp | |
| updated_at | timestamp | |

This enables:
- Multiple URLs per client
- Named scrape targets
- URL-level lifecycle tracking

---

## 3ï¸âƒ£ `scrape_jobs` Table (Update)

Instead of referencing `client_id`, reference `client_url_id`.

| Field | Type | Notes |
|-------|------|-------|
| id | PK | |
| client_url_id | FK â†’ client_urls.id | Specific URL scraped |
| status | string | pending / running / complete / failed |
| started_at | timestamp | |
| completed_at | timestamp | |
| result_path | string | Annotated PDF output |

This ensures:
- Exact traceability of what was scraped
- Proper job auditing
- URL-specific failure tracking

---

# ğŸ–¥ï¸ UI Changes Required

## Client Management
- Client form must allow adding multiple URLs
- URL entries require:
  - alias
  - URL
  - active toggle
- Support edit/delete per URL

## Scraper Job Creation
- User selects:
  - Client
  - Specific Client URL (by alias)

---

# ğŸ”„ Migration Plan

1. Create `client_urls` table
2. Migrate existing `website_url` values into `client_urls`
   - alias = "default"
   - is_active = true
3. Drop `website_url` column from `clients`
4. Update foreign key references in `scrape_jobs`
5. Update forms and API endpoints

---

# ğŸš€ Long-Term Benefits

This design supports:

- URL-specific scheduling
- Parallel scraping
- URL-level analytics
- Failure monitoring
- Retry logic per URL
- Cleaner scaling model

---

# ğŸ§  Design Principle

Do not keep `website_url` for backward compatibility.

This project is still in development.
Avoid carrying legacy schema forward.

Build it correctly now.

---

# âœ… Final Decision Summary

- âœ” Create `client_urls` table
- âœ” Remove `website_url` from clients
- âœ” Update scrape_jobs to reference client_url_id
- âœ” Update UI to manage multiple URLs
- âœ” Migrate existing data
